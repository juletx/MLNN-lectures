<!doctype html>
<html lang="en">


   
    
	<head>
		<meta charset="utf-8">

		<title>Machine Learning and Deep Learning</title>
                <meta name="author" content="Roberto Santana">
		<meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=1.0, user-scalable=no">

		<!-- <link rel="stylesheet" href="css/reveal.css">  -->
                <link rel="stylesheet" href="css/fullscreen-img.css">
                <link rel="stylesheet" href="css/added_css/notebook.css">
   	        <link rel="stylesheet" href="css/reveal.css">
                <link rel="stylesheet" href="css/theme/nncourse.css" id="theme">
                                

	</head>

	<body>


		<div class="reveal">
			<div class="slides">

				<section>
                                          <div class="my_container">
                                        <h2>Machine Learning and Neural Networks</h2>
					<p>Roberto Santana and Unai Garciarena<p>
					<p>Department of Computer Science and Artificial Intelligence</p>
                                        <p>University of the Basque Country</p>
                          		 </div>   
				</section>




                                 <section>
                                        <div class="my_container">
                                        <h3>Table of Contents </h3>
                                        
                                         <table style="width:100%"; border=solid>

                                           <tr>

					     <td> <p class="paragraph2"> <a href="#/sec:ML_Definitions"> Task, Experience, Performance </a></p></td>
                                             <td> <p class="paragraph2"> <a href="#/sec:ML_S_SEARCH_CLASS"> Learning and optimization </a></p></td>
                                              <td> <p class="paragraph2"> <a href="#/sec:ML_S_CLF"> Linear models for supervised learning </a></p></td>
			         
                                                
                                                    
                                           </tr>
					   
							  
					    <tr>

					         <td> <p class="paragraph2"> <a href="#/sec:ML_S_LR"> Linear regression </a></p></td>      

                                               
			                  
                                            </tr>
										      
					   
					   										      
					

				         </table>	  
                          	     </div>   

				</section>

				 <section>
    				      <section id="sec:ML_Definitions">                   
                                                 <div class="my_container">            	                
                           		         <h3>MLNN course</h3>
                           		         <h4>Recommended bibliography</h4>
                                                  
                                                     <ul>  
                                                       <li class="paragraph2"> I. Goodfellow and Y. Bengio and A. Courville. <a href="http://www.deeplearningbook.org/"> Deep Learning.</a> Chapter. 5. <a href="https://www.deeplearningbook.org/contents/ml.html"> Machine Learning Basics.</a>. MIT Press. 2016.  </li>

 					          <li class="paragraph2">  K. P. Murphy. <a href="https://www.cs.ubc.ca/~murphyk/MLbook/"> Machine learning. A probabilistic perspective.</a> MIT Press. 2012. </li>

 					          <li class="paragraph2">  (Auxiliar bibliography) G. Thomas. <a href="https://gwthomas.github.io/docs/math4ml.pdf"> Mathematics for Machine Learning. </a>  2018. </li>

				

						  
						     </ul>

                                                     
                          		 </div>                                     
                                                   <aside class="notes">
                                                       What is ML? 
                                            	   </aside>

                             	  </section>

				   <section id="sec:ML_Definitions">
				  

                                                 <div class="my_container">            	                
                           		         <h3>What is machine learning?</h3>
                           		         <h4>Definitions</h4>
                                                  
                                                     <ul>  
                                                       <li class="paragraph2"> The capacity of a computer to  <mark class="red">learn from experience</mark>, i.e., to modify its processing on the basis of newly acquired information. (Oxford dictionary).</li>

      		           			         <li class="paragraph2"> ML is the field of study that gives computers the <mark class="red">ability to learn</mark>  without being explicitly programmed (Samuel:1959).</li>
       
                                                         <li class="paragraph2"> Make the computer  <mark class="red">to adapt</mark>  to new circumstances and <mark class="red">to detect and extrapolate</mark>  patterns. </li>
							 
							 <li class="paragraph2">A computer program is said to  <mark class="red">learn from experience</mark>  E with respect to some task T and some performance measure P, <mark class="red">if its performance</mark>  on T, as measured by P, <mark class="red">improves with experience</mark> E (Mitchell:1997). </li>

 
                                                          </ul>

                                      		 </div>
						 <p class="paragraph2"> I. Goodfellow and Y. Bengio and A. Courville. <a href="http://www.deeplearningbook.org/"> Deep Learning.</a>. MIT Press. 2016. </p>	                                                   
                          	
                     
                             	  </section>


				  <section>                   
                                                 <div class="my_container">            	                
                           		         <h3>What is machine learning?</h3>
                           		         <h4>Definitions</h4>

                                                     <ul>                                                             
     		           			           <li class="paragraph2">A computer program is said to  <mark class="red">learn from experience</mark>  E with respect to some task T and some performance measure P, <mark class="red">if its performance</mark>  on T, as measured by P, <mark class="red">improves with experience</mark> E (Mitchell:1997). </li>                                                      
                                                     </ul>
						  <span class="fragment">   
						  <h4>Components</h4>
                                                     <ol>  
                                                           <li class="paragraph2"> The experience  <mark class="red">E</mark>.</li>
      		           			           <li class="paragraph2"> The task <mark class="red" >T</mark> .</li>
                                                           <li class="paragraph2"> The performance measure <mark class="red" >P</mark>.</li>						
                                                     </ol>
					

                                                                                            

                          			 </div>
						                <p class="paragraph2"> I. Goodfellow and Y. Bengio and A. Courville. <a href="http://www.deeplearningbook.org/"> Deep Learning.</a>. MIT Press. 2016. </p>	                                                 
                                                   <aside class="notes">
                                                       What is ML? 
                                            	   </aside>

                             	  </section>

				  <section>                   
                                             <div class="my_container">
   				                 <mark class="red"></mark>
                           		         <h3>Experience E</h3>

                                                     <ul>                                                             
     		           			       <li class="paragraph2"> Most of ML algorithms that we will consider understand or represent experience as a <mark class="red">dataset of examples</mark>. </li>
     		           				 <li class="paragraph2"> An example is <mark class="red">a collection of features</mark> that have been quantitatively measured from some object or event that we want the ML system to process.</li>
     		           				 <li class="paragraph2"> We usually represent an example as  <mark class="red"> a vector \( {\bf{x}} \in \mathcal{R}^n \)</mark>, where each entry \(x_i\) is a feature. </li>
							 <li class="paragraph2"> One common way of representing a database is with a <mark class="red">design matrix</mark>, where each row represents an example and each column a feature.</li>
     		           				 <li class="paragraph2"> For supervised classification problems, the class or target variable is part of the experience.</li> 
					
						       
                                                    </ul>
						                                                    	                                                 
                                                     

                          		     </div>
					        <p class="paragraph2"> I. Goodfellow and Y. Bengio and A. Courville. <a href="http://www.deeplearningbook.org/"> Deep Learning.</a>. MIT Press. 2016. </p>
                                                   <aside class="notes">
                                                       What is ML? 
                                            	   </aside>

                             	  </section>


				  <section>                   
                                             <div class="my_container">
   				                 <mark class="red"></mark>
                           		         <h3>Task T</h3>

                                                     <ul>                                                             
     		           			       <li class="paragraph2"> The task is the final problem the ML is intended to solve. The same task can be solved  <mark class="red">using different ML algorithms</mark>.</li>					     
						     </ul>

			                          <span class="fragment">
						  <h4>Examples of tasks</h4>

                                                     <ul>                                                             
     		           			       <li class="paragraph2"><mark class="red">Classification</mark>: The ML algorithm is asked to specify which of k categories some example belongs to.</li>

						       							 
						  </span>
			                          <span class="fragment">

						    <li class="paragraph2"><mark class="red">Regression</mark>: To predict a numerical value given some example.</li>

						  </span>
			                          <span class="fragment">
     		           			    <li class="paragraph2"><mark class="red">Transcription</mark>: Observe a relatively unstructured representation of some kind of data and transcribe it to a discrete or textual form.</li>

						   </span>
			                          <span class="fragment">
						       <li class="paragraph2"><mark class="red">Translation</mark>: Given a sequence of symbols in some language, transcribe it to another language.</li>    		           			    
						      
						   </span> 
						     </ul>
						                                                    

                          		     </div>
					       <p class="paragraph2"> I. Goodfellow and Y. Bengio and A. Courville. <a href="http://www.deeplearningbook.org/"> Deep Learning.</a>. MIT Press. 2016. </p>	                                          
                                                   <aside class="notes">
                                                       What is ML? 
                                            	   </aside>

                                  </section>
				  <section>                   
                                             <div class="my_container">
   				                 <mark class="red"></mark>
                           		         <h3>Task T</h3>

						  <h4>More examples of tasks</h4>


                                                     <ul>                                                             
     		           			       <li class="paragraph2"><mark class="red">Structured output</mark>: Involves any task where the output is a vector (or other data structure containing multiple values) with important relationships between the elements.</li>

						    <li class="paragraph2"><mark class="red">Anomaly detection</mark>: Given a set of objects or events, the task is to identify some of them as unusual or atypical.</li>

     		           			    <li class="paragraph2"><mark class="red">Synthesis and sampling</mark>: To generate new examples that are similar to those in the training data.</li>

						       <li class="paragraph2"><mark class="red">Denoising</mark>: The algorithm is given a corrupted example  \( \tilde{{\bf{x}}} \in \mathcal{R}^n \)  and it should output the clean example  \( {\bf{x}} \in \mathcal{R}^n \).</li>    		           			    
						       
				        
                                                     </ul>				 

                                                                                                    
                                                     

                          		     </div>
					     <p class="paragraph2"> I. Goodfellow and Y. Bengio and A. Courville. <a href="http://www.deeplearningbook.org/"> Deep Learning.</a>. MIT Press. 2016. </p>	    
                                                   <aside class="notes">
                                                       What is ML? 
                                            	   </aside>

                             	  </section>

				  <section>                   
                                             <div class="my_container">
   				                 <mark class="red"></mark>
                           		         <h3>Performance Measure P</h3>

                                                     <ul>                                                             
     		           			       <li class="paragraph2"> A  <mark class="red">quantitative measure of the performance</mark> of the ML algorithm. Usually, <mark class="red">P is specific to the task T</mark>.</li>					     
						     </ul>


						  <h4>Examples of performance measures</h4>

                                                     <ul>                                                             
     		           			       <li class="paragraph2"><mark class="red">Classification</mark>: Usually, the  <mark class="red">accuracy </mark> (proportion of examples for which the model produces the <mark class="red">correct</mark> output).</li>

     		           			       <li class="paragraph2"><mark class="red">Classification</mark>: Also, the <mark class="red">error rate</mark> (proportion of examples for which the model produces the <mark class="red">incorrect</mark> output).</li>				       

						    <li class="paragraph2"><mark class="red">Regression</mark>: A measure of the distance between the prediction, and the target variable, for example the <mark class="red">mean squared error </mark> .</li>

						       <li class="paragraph2"><mark class="red">Denoising</mark>: The amount of corruption that has been removed from the original example.</li>    		          			    
						    
                                                     </ul>
						 
                                                    

                          		     </div>
                                                <p class="paragraph2"> I. Goodfellow and Y. Bengio and A. Courville. <a href="http://www.deeplearningbook.org/"> Deep Learning.</a>. MIT Press. 2016. </p>	                                                 

                                                   <aside class="notes">
                                                       What is ML? 
                                            	   </aside>

                                  </section>
			      </section>

			       <section  id="sec:ML_S_SEARCH_CLASS">
                                      <section>
                                                 <div class="my_container">
                                                   <h3>Supervised learning</h3>                                                   
                                                   <h4>Learning as a search process</h4>
                                                   <ol>

      		 				      <li class="paragraph2"><a href="#/sec:ML_S_CL_LEARNING"> Learning methods </a>  </li>
						      <li class="paragraph2"><a href="#/sec:ML_S_CL_OPT"> Optimization algorithms </a> </li>                                                    </ol>						                                                                                     </div>                                                         
                                                   
                                                   <aside class="notes">
  					
                                            	   </aside>
        			      </section>

                                     <section id="sec:ML_S_CL_LEARNING">
                                          <div class="my_container">                             	        
                                 	  <h3>Classifiers</h3>
                                 	  <h4>The three components of learning algorithms</h4>


                                           <ol>
                                              <li class="paragraph2"> <mark class="red">Representation</mark>: Choosing a representation for a classifier influence to a large extent the set of classifiers that it can learn. This set is called the hypothesis space of the learner.</li>
                                              <li class="paragraph2"><mark class="red">Evaluation</mark>: To distinguish between good and poor classifiers at least one  criterion is required that is usually evaluating using an objective function or scoring function.  </li>
                                              <li class="paragraph2"><mark class="red">Optimization</mark>: The search for the best classifier in the hypothesis space is usually posed as an optimization problem. The choice of the optimization technique is key to the efficiency of the learner. </li>
                                            
                                           </ol>             
                                                                         
                                                    
                          		  </div>
					   <p class="paragraph2">P. Domingos.  <a href="http://www.idi.ntnu.no/emner/tdt4173/papers/domingos-cacm12.pdf"> A few useful things to know about machine learning.</a> Communications of the ACM, 55(10), 78-87. 2012.</p>                  
                                           <aside class="notes">
                                            Probability analysis of the classification problem
                                           </aside>

 				    </section>   
                                    <section>
                                        <div class="my_container">
                                 	  <h4>The three components of learning algorithms</h4>
                                      
                                        
                                         <table style="width: 100%;">
                                                  <tr>
                                                      <td><p class="paragraph3"><mark class="red">Representation</mark></p></td>
                                                      <td><p class="paragraph3"><mark class="red">Evaluation</mark></p></td>
                                                      <td><p class="paragraph3"><mark class="red">Optimization</mark></p></td>
                                                  </tr>
                                                  <tr>
                                                      <td><p class="paragraph3"><mark class="green">Instances</mark></p></td>
                                                      <td><p class="paragraph3">Accuracy/Error rate</p></td>
                                                      <td><p class="paragraph3"><mark class="violet">Combinatorial optimization</mark></p></td>
                                                  </tr>
                                                  <tr>
                                                      <td><p class="paragraph3">--K-nearest neighbor</p></td>
                                                      <td><p class="paragraph3">Precision and recall</p></td>
                                                      <td><p class="paragraph3">--Greedy search</p></td>
                                                  </tr>
                                                  <tr>
                                                      <td><p class="paragraph3">-- Support vector machines</p></td>
                                                      <td><p class="paragraph3">Squared error</p></td>
                                                      <td><p class="paragraph3">--Beam search</p></td>
                                                  </tr>
                                                  <tr>
                                                      <td><p class="paragraph3"><mark class="green">Hyperplanes</mark></p></td>
                                                      <td><p class="paragraph3">Likelihood</p></td>
                                                      <td><p class="paragraph3">--Branch-and-bound</p></td>
                                                  </tr>
                                                  <tr>
                                                      <td><p class="paragraph3">--Naive Bayes</p></td>
                                                      <td><p class="paragraph3">Posterior probability</p></td>
                                                      <td><p class="paragraph3"><mark class="violet">Continuous optimization</mark></p></td>
                                                  </tr>
                                                  <tr>
                                                      <td><p class="paragraph3">--Logistic regression</p></td>
                                                      <td><p class="paragraph3">Information gain</p></td>
                                                      <td><p class="paragraph3">-Unconstrained</p></td>
                                                  </tr>
                                                  <tr>
                                                      <td><p class="paragraph3"><mark class="green">Decision trees</mark></p></td>
                                                      <td><p class="paragraph3">KL divergence</p></td>
                                                      <td><p class="paragraph3">--Gradient descent</p></td>
                                                  </tr>
                                                  <tr>
                                                      <td><p class="paragraph3"><mark class="green">Set of rules</mark></p></td>
                                                      <td><p class="paragraph3">Cost/Utility</p></td>
                                                      <td><p class="paragraph3">--Conjugate gradient</p></td>
                                                  </tr>
                                                  <tr>
                                                      <td><p class="paragraph3">--Propositional rules</p></td>
                                                      <td><p class="paragraph3">Margin</p></td>
                                                      <td><p class="paragraph3">--Quasi-Newton methods</p></td>
                                                  </tr>
                                                  <tr>
                                                      <td><p class="paragraph3">--Logic programs</p></td>
                                                      <td><p class="paragraph3"></p></td>
                                                      <td><p class="paragraph3">-Constrained</p></td>
                                                  </tr>
                                                  <tr>
                                                      <td><p class="paragraph3"><mark class="green">Neural networks</mark></p></td>
                                                      <td><p class="paragraph3"></p></td>
                                                      <td><p class="paragraph3">--Linear programming</p></td>
                                                  </tr>
                                                  <tr>
                                                      <td><p class="paragraph3"><mark class="green">Graphical models</mark></p></td>
                                                      <td><p class="paragraph3"></p></td>
                                                      <td><p class="paragraph3">--Quadratic programming</p></td>
                                                  </tr>
                                                  <tr>
                                                      <td><p class="paragraph3">--Bayesian networks</p></td>
                                                      <td><p class="paragraph3"></p></td>
                                                      <td><p class="paragraph3"></p></td>
                                                  </tr>
                                                              
				              </table>	  
                          	              </div>   
			            </section>


				                                    <section>
                                          <div class="my_container">

                             	          <h3>Classification problems</h3>              
                                 	  <h4>Probability analysis</h4>                                        
                                                                          
                          		      <p class="paragraph2"> Let \( p({\bf{x}}) \) be a probability distribution defined on a discrete feature  \( {\bf{x}} \). \( p({\bf{x}}) \) satisfies the following: </p>

                                              <p class="paragraph2"> 
                                                \[
                                                      p[{\bf{X}} = {\bf{x}}] = p({\bf{x}})
                                                   
                                                \] 
                                              </p>        

                                               <p class="paragraph2"> 
                                                \[ 
                                                   p({\bf{x}}) \geq 0 \; \; \forall  {\bf{x}} 
                                                \] 
                                               </p>   

                                               <p class="paragraph2">                                 
                                                \[
                                                      \sum_{{\bf{x}}}  p({\bf{x}}) = 1                                                  
                                                \] 
                                              </p>        
                          		 </div>   
                	               		 

                                           <aside class="notes">
                                                 Originally used to describe the probability distribution of particles in a system over various possible states.
                                           </aside>

 				      </section>    
 


                                     <section>
                                          <div class="my_container">
                             	          <h3>Classification problems</h3>              
                                 	  <h4>Probability analysis</h4>

                                           <ul>

                                              <li class="paragraph2"> Let  \( P(C_1) \) and \( P(C_2) \) be the probabilities we know a priori that an observation belongs to clases  \( C_1 \) and \( C_2 \), respectively. </li>
                                              <li class="paragraph2"> \( P(C_i,{\bf{x}}) \) is the <mark class="red">joint probability</mark>  that  \({\bf{X}} \) takes value  \( {\bf{x}} \) and belongs to class  \( C_i \) </li>
                                              <li class="paragraph2"> We define  \( P({\bf{x}}|C_i) \) as the  <mark class="red">conditional probability </mark>  that \( {\bf{X}} \)  takes value \( {\bf{x}} \) given that it belongs to class  \( C_i \)  </li>
                                             
                                           <ul/>                                        
                                                    
                          		 </div>  
                                           <aside class="notes">
                                            Probability analysis of the classification problem
                                           </aside>

 				    </section>    
                                    <section>
                                          <div class="my_container">
                             	          <h3>Classification problems</h3>              
                                 	  <h4>Probability analysis</h4>
                                           <mark class="red"></mark>
                                           <ul>
                                              <li class="paragraph2"> We know that   \( P(C_i,{\bf{x}}) =  P({\bf{x}}|C_i) P(C_i) \) and  \( P(C_i,{\bf{x}}) =  P(C_i|{\bf{x}}) P({\bf{x}}) \). </li>                                        
                                              <li class="paragraph2"> Therefore,
                                               \[
                                                     P(C_i|{\bf{x}}) = \frac{P({\bf{x}}|C_i) P(C_i)}{P({\bf{x}})}
                                               \]
                                              </li>
                                              <li class="paragraph2"> This expression is referred as <mark class="red">Bayes's theorem</mark>. </li>
                                              <li class="paragraph2"> \(P(C_i) \) is known as the <mark class="red">prior probability</mark>. </li>
                                              <li class="paragraph2"> \( P({\bf{x}}|C_i) \) is the <mark class="red">class conditional probability</mark> of  \(P({\bf{x}}) \) for class  \( C_i \). </li>
                                              <li class="paragraph2">  \(P(C_i|{\bf{x}})\) is the <mark class="red">posterior probability</mark></li>
                                           <ul/>                                        
                                                    
                          		 </div>  
                                           <aside class="notes">
                                              The importance of Baye's theorem lies int the fact that it re-expresses the posterior probabilities in terms of quantities which are more often much easier to calculate. 
                                              We can use the theorem also for problems where the features are continuous and the class conditional probability is a density function.
                                           </aside>

 				      </section>    
                                    <section>
                                          <div class="my_container">
                             	          <h3>Classification problems</h3>              
                                 	  <h4>Probability analysis</h4>

                                           <ul>

                                              <li class="paragraph2"> The posterior probability  \( P(C_i|{\bf{x}}) \) gives the probability of a sample belonging to class  \( C_i \) once we have observed the feature vector  \( {\bf{x}} \). </li>
                                              <li class="paragraph2"> The probability of misclassification is minimized by selecting the clas  \( C_i \)  having the largest posterior probability, i.e., 
                                               \[
                                                     P(C_i|{\bf{x}}) > P(C_j|{\bf{x}}) \; \; \forall j \neq i.
                                               \]
                                              </li>              
                                           <ul/>                                        
                                                    
                          		 </div>  
                                           <aside class="notes">
                                            Probability analysis of the classification problem
                                           </aside>

 				    </section>  
                                    <section>
                                          <div class="my_container">
                             	          <h3>Classification problems</h3>              
                                 	  <h4>Classifiers</h4>

                                           <ul>

                                              <li class="paragraph2"> According to the largest posterior probability \( P(C_i|{\bf{x}}) \), a  classifier provides a rule for assigning each point of the feature space to one of \(k\) classes. </li>
                                              <li class="paragraph2"> Therefore, a classifier divides the feature space into \( k \) decision regions  \( \{ \mathcal{R}_1, \dots,  \mathcal{R}_k \} \). </li>
                                               <li class="paragraph2"> We can extend the idea of using posterior probabilities for each class by defining a set of <mark class="red">discriminant functions</mark>  \( y_1({\bf{x}}), \dots,  y_k({\bf{x}}) \) such that an input vector  \( {\bf{x}} \) is assigned to class \( C_i \) if
                                               \[
                                                     y_i({\bf{x}}) > y_j({\bf{x}}) \; \; \forall j \neq i.
                                               \]
                                              </li>             
                                            <ul/>                                        
                                                    
                          		   </div>  
                                           <aside class="notes">
                                                                Probability analysis of the classification problem
                                           </aside>

        				   </section>    

                                          <section>
                                                 <mark class="red"></mark>
                                                 <div class="my_container">                             	                   
                                                   <h3>Supervised learning</h3>   
                                                   <h4>Generative versus discriminative classifiers</h4>
                                                   <ol>

						          <li class="paragraph2"><mark class="red">Generative</mark>: Provide a model of how the observations can be generated given the class. </li>
						          <li class="paragraph2"><mark class="red">Discriminative</mark>: Does not provide a model but allow the discrimination of the observation according to the classes.</li>

						          <li class="paragraph2">  Discriminative models learn the boundary between classes while generative models model the distribution of individual classes.</li>
						          <li class="paragraph2">Examples of generative models: <mark class="red"></mark>naive Bayes. </li>
	 				                  <li class="paragraph2">Examples of discriminative models: <mark class="red"></mark>decision trees. </li>
                                                   <mark class="red"></mark>
                                                   </ol>						           
                               		           </div>                                                       
                                                   <aside class="notes">
  						   
                                           	   </aside>
        			          </section>
   


                              </section>

                                <section>
				       <section id="sec:ML_S_CLF">
                                                 <div class="my_container">
                                                   <h3>Supervised learning</h3>                                                   
                                                   <h4>Regressors and classifiers</h4>
                                                   <ol>

 <td> <p class="paragraph2"> <a href="#/sec:ML_NNs">  </a></p></td>  

						          <li class="paragraph2"><a href="#/sec:ML_S_LR"> Linear regression </a> </li>
                                                          <li class="paragraph2"><a href="#/sec:ML_S_LDA"> Linear Discriminant Analysis </a> </li>
						          <li class="paragraph2"><a href="#/sec:ML_S_LOGR"> Logistic regression  </a> </li>
      		 		
                                                   
                                                   </ol>						           
                                                   

                          		         </div>
						    <p class="paragraph2"> C. M. Bishop.  <a href="http://cs.du.edu/~mitchell/mario_books/Neural_Networks_for_Pattern_Recognition_-_Christopher_Bishop.pdf">Neural Networks for Pattern Recognition.</a>  Oxford University Press. 2005.</p>                                 
                                                    <p class="paragraph2"> P. Domingos. <a href="http://www.idi.ntnu.no/emner/tdt4173/papers/domingos-cacm12.pdf"> A few useful things to know about machine learning.</a> Communications of the ACM, 55(10), 78-87. 2012.</p>                  
     
                                                   
                                                   <aside class="notes">
  					
                                            	   </aside>
        			          </section>



                                         <section id="sec:ML_S_LR" data-transition="none">     
                                           <h3>Supervised learning</h3>       
                                	  <h4>Regression</h4>                                                 
                                                <div class="container">
                                                                                          
                                                   <ul>                                                                                                                              <p class="paragraph2"><img src="href=../../imgl2/classregions/Regression_1.png"  height="420" width="500"></p>                            
                                                                                                                                                                                                           <ul/>                                                                  
                                                 </div>            
                                                                                                                    

                                
                                                   <aside class="notes">
                                                       Classification problems can vary in their complexity.                   
                                            	   </aside>
        		  	         </section>
                                         <section data-transition="none">                                                             		               <h3>Supervised learning</h3>       
                                	  <h4>Regression</h4>                                                 
                                                <div class="container">

                                                   <ul>                                                                                                                              <p class="paragraph2"><img src="href=../../imgl2/classregions/Regression_2.png"  height="420" width="500"></p>                            
                                                                                                                                                                                                           <ul/>                                                                  
                                                 </div>            
                                                                                                                    

                                
                                                   <aside class="notes">
                                                       Classification problems can vary in their complexity.                   
                                            	   </aside>
        		  	         </section>

			 
                                   <section>
                                          <div class="my_container">

                             	          <h3>Supervised learning</h3>              
                                 	  <h4>Linear regression</h4>                                        

					     <p class="paragraph2"> A set of \(N\) tuples  \( (x^1,y^1), \dots (x^N,y^N) \) is given, where \(y\) is the target or dependent variable and \(x\) is the covariate, independent variable or predictor. The task is to predict  \(y\) given  \(x\). </p></p>
                                            
					     
					  
                          	             <p class="paragraph2"> <mark class="red">General regression model</mark>:
                                                \[
                                                      y  =  f(x) + \epsilon 
                                                   
                                                \] 
                                             </p>

                                             <p class="paragraph2"> where \( \epsilon \) is the irreducible error that does not depend on x. </p>

					  
                          	             <p class="paragraph2"> <mark class="red">Linear regression model</mark>:
                                                \[
                                                      f(x) =  \beta_1 x + \beta_0
                                                   
                                                \] 
                                             </p>
					     
					      <p class="paragraph2"> <mark class="red"> Linear regression estimate</mark>:
                                                \[
                                                      \hat{y}  =  \hat{\beta_1} x + \hat{\beta_0}
                                                   
                                                \] 
                                              </p>        

					       <p class="paragraph2"> The<mark class="red"> residual error</mark> is the difference between the prediction and the true value.
                                                \[
                                                      e^i  = y^i - \hat{y}^i   
                                                   
                                                \] 
                                             </p>


                              		     </div>   

 	               		              <p class="paragraph2">   K. P. Murphy. <a href="https://mitpress.mit.edu/books/machine-learning-0"> Machine learning. A probabilistic perspective.</a> MIT Press. 2012. </p>                                                                                                

                                               <aside class="notes">                                                       
                                               </aside>

                                   </section>
                                   <section>
                                          <div class="my_container">

                             	          <h3>Supervised learning</h3>              
                                 	  <h4>Linear regression</h4>                                        

					      
					      <p class="paragraph2">  The <mark class="red">mean squared error</mark> is usually used:
                                                \[
                                                      MSE  = \frac{1}{N} \sum_{i=1}^N  (y^i - \hat{y}^i)^2 
                                                   
                                                \] 
                                              </p>

					  <p class="paragraph2">  The  <mark class="red">parameters of the model</mark>  that minimize this error are learned:

					        \[
                                                    \arg \min_{\beta_0,\beta_1} \frac{1}{N} \sum_{i=1}^N  (y^i - (\beta_1 x^i + \beta_0))^2 
                                                   
                                                \] 
                                          
                                              </p>  

					      

                                          <p class="paragraph2"> After differentiating with respect to \( \beta_0,\beta_1 \) and equalling to \(0\), we get:

					   \[
                                                    \hat{\beta_0} = \bar{y} - \hat{\beta_1}\bar{x}; \; \;  \hat{\beta_1} = \frac{\sum_{i=1}^N x^i y^i - N \bar{x} \bar{y}} {\sum_{i=1}^N (x^i)^2  - N \bar{x}^2}  
                                                   
                                            \]

					    where \(\bar{x}\) and \(\bar{y}\) are the mean of \(x\) and  \(y\)  as computed from the data.
					  </p>  

                              		     </div>   

 	               		              <p class="paragraph2">   K. P. Murphy. <a href="https://mitpress.mit.edu/books/machine-learning-0"> Machine learning. A probabilistic perspective.</a> MIT Press. 2012. </p>                                                                                                


                                               <aside class="notes">
                                                       
                                               </aside>

 				   </section>


				   

                                   <section>
                                          <div class="my_container">

                             	          <h3>Supervised learning</h3>              
                                 	  <h4>Multiple linear regression</h4>                                        

					  <p class="paragraph2"> In multiple linear regression we have multiple covariates, represented as a vector  \({\bf{x}} \). The model is linear on the covariates. </p>

					   <p class="paragraph2"> <mark class="red">Multiple linear regression model</mark>:
                                                \[
                                                      f(x) =   \beta_0 + \beta_1 x_1 + \beta_2 x_2  + \dots  + \beta_n x_n + \epsilon
                                                   
                                                \] 
                                             </p>
                          		      <p class="paragraph2"> Let \( {\bf{w}} \) be the model weight vector containing \(\beta\) values, ,   \( {\bf{w}}^T {\bf{x}} \) represents the inner or scalar product between the input vector \( {\bf{x}} \) and the weight vector. </p>

                                              <p class="paragraph2"> Then, the  <mark class="red"> multiple linear regression model </mark> in matrix form is expressed as:
                                                \[
                                                      y({\bf{x}}) = {\bf{w}}^T {\bf{x}} + \epsilon = \sum_{j=1}^{n} w_jx_j + \epsilon 
                                                   
                                                \] 
                                              </p>        

					      <p class="paragraph2"> Estimates of \(w\) are found by <mark class="red"> minimizing the MSE</mark> in a way similar to the case of a single covariate. That way the parameters of the model are learned. </p>

                              		     </div>   

 	               		              <p class="paragraph2">   K. P. Murphy. <a href="https://mitpress.mit.edu/books/machine-learning-0"> Machine learning. A probabilistic perspective.</a> MIT Press. 2012. </p>                                                                                                


                                               <aside class="notes">
                                                       
                                               </aside>

 				           </section>   
                                         
            		    
                           </section>
 


			</div>
		</div>





		<script src="lib/js/head.min.js"></script>
		<script src="js/reveal.js"></script>

		<script>

			Reveal.initialize({
				history: true,
				transition: 'linear',

				math: {
					// mathjax: 'http://cdn.mathjax.org/mathjax/latest/MathJax.js',
					config: 'TeX-AMS_HTML-full'
				},

				dependencies: [
                                        { src: 'lib/js/fullscreen-img.js' },
					{ src: 'lib/js/classList.js' },
					{ src: 'plugin/math/math.js', async: true }

				]
			});

		</script>

	</body>
</html>
